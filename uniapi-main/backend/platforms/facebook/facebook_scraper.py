"""
Facebook Scraper - å®Œæ•´ç‰ˆ
å‚ç…§Reddit/Twitteræ¨¡å¼ï¼šæœç´¢å…³é”®è¯ â†’ æ‰¾å¸–å­ â†’ æŠ“ç”¨æˆ·
"""

import json
import time
import random
import logging
from typing import List, Dict, Optional
from playwright.sync_api import sync_playwright
from src.platform_scraper_base import PlatformScraperBase

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class FacebookScraper(PlatformScraperBase):
    """Facebookçˆ¬è™« - æœç´¢å¸–å­å¹¶æŠ“å–äº’åŠ¨ç”¨æˆ·"""

    def __init__(self, auth_file: str = "platforms_auth.json"):
        """åˆå§‹åŒ–"""
        with open(auth_file, 'r') as f:
            config = json.load(f)

        super().__init__(config.get('facebook', {}), 'Facebook')
        self.cookies = self.auth_config.get('cookies', {})

        self.playwright = None
        self.browser = None
        self.page = None

    def _start_browser(self):
        """å¯åŠ¨æµè§ˆå™¨ï¼ˆå¸¦åæ£€æµ‹ï¼‰"""
        if self.browser:
            return

        logger.info("ğŸš€ Starting Facebook browser...")

        self.playwright = sync_playwright().start()

        # ä½¿ç”¨æ›´å¥½çš„åæ£€æµ‹è®¾ç½®
        self.browser = self.playwright.chromium.launch(
            headless=False,
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage',
                '--no-sandbox',
            ]
        )

        self.context = self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            locale='en-US',
            timezone_id='America/Los_Angeles',
        )

        # éšè—webdriverç‰¹å¾
        self.context.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined
            });
        """)

        # åŠ è½½cookies
        if self.cookies:
            cookies_list = [{
                'name': name,
                'value': value,
                'domain': '.facebook.com',
                'path': '/'
            } for name, value in self.cookies.items()]
            self.context.add_cookies(cookies_list)

        self.page = self.context.new_page()
        logger.info("âœ… Browser started")

    def _close_browser(self):
        """å…³é—­æµè§ˆå™¨"""
        try:
            if self.browser:
                self.browser.close()
            if self.playwright:
                self.playwright.stop()
            logger.info("ğŸ”’ Browser closed")
        except Exception as e:
            # å¿½ç•¥å…³é—­æ—¶çš„é”™è¯¯
            pass

    def search_posts_from_groups(self, group_ids: List[str], max_posts_per_group: int = 10) -> List[Dict]:
        """
        ä»Facebookç¾¤ç»„è·å–å¸–å­ï¼ˆé¿å¼€æœç´¢é¡µé¢çš„å´©æºƒé—®é¢˜ï¼‰

        Args:
            group_ids: ç¾¤ç»„IDåˆ—è¡¨æˆ–ç¾¤ç»„åç§°åˆ—è¡¨
            max_posts_per_group: æ¯ä¸ªç¾¤ç»„æœ€å¤šæŠ“å–å¸–å­æ•°

        Returns:
            å¸–å­åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« {url, author, group}
        """
        self._start_browser()

        logger.info(f"ğŸ” Fetching posts from {len(group_ids)} Facebook groups...")

        all_posts = []

        for group_id in group_ids:
            logger.info(f"\nğŸ“Œ Group: {group_id}")

            try:
                # è®¿é—®ç¾¤ç»„é¡µé¢
                group_url = f"https://www.facebook.com/groups/{group_id}"
                self.page.goto(group_url, wait_until='domcontentloaded', timeout=60000)
                time.sleep(random.uniform(3, 5))

                # æ£€æŸ¥ç™»å½•
                if "login" in self.page.url:
                    logger.error("âŒ Not logged in")
                    continue

                # æ»šåŠ¨åŠ è½½å¸–å­
                logger.info("   ğŸ“œ Loading posts...")
                for _ in range(3):
                    self.page.evaluate("window.scrollBy(0, 800)")
                    time.sleep(random.uniform(1, 2))

                # æŸ¥æ‰¾å¸–å­é“¾æ¥
                posts_in_group = []
                seen_urls = set()

                # Facebookç¾¤ç»„å¸–å­é“¾æ¥æ¨¡å¼
                post_link_selectors = [
                    'a[href*="/posts/"]',
                    'a[href*="/permalink/"]',
                ]

                for selector in post_link_selectors:
                    try:
                        links = self.page.query_selector_all(selector)

                        for link in links:
                            if len(posts_in_group) >= max_posts_per_group:
                                break

                            href = link.get_attribute('href')
                            if not href:
                                continue

                            # è¡¥å…¨URL
                            if href.startswith('/'):
                                href = f'https://www.facebook.com{href}'

                            # å»é‡
                            if href in seen_urls:
                                continue

                            # åªè¦ç¾¤ç»„å†…çš„å¸–å­
                            if f'/groups/{group_id}' in href and '/posts/' in href:
                                seen_urls.add(href)

                                posts_in_group.append({
                                    'url': href,
                                    'author': 'Unknown',  # å¯ä»¥åç»­æå–
                                    'group_id': group_id,
                                    'platform': 'facebook'
                                })

                    except Exception as e:
                        logger.debug(f"   Error with selector {selector}: {e}")
                        continue

                logger.info(f"   âœ… Found {len(posts_in_group)} posts in group")
                all_posts.extend(posts_in_group)

                # ç¾¤ç»„é—´å»¶è¿Ÿ
                time.sleep(random.uniform(2, 4))

            except Exception as e:
                logger.error(f"   âŒ Error: {e}")
                continue

        logger.info(f"\nâœ… Total: {len(all_posts)} posts from {len(group_ids)} groups")
        return all_posts

    def get_post_comments(self, post_url: str, max_comments: int = 50) -> List[Dict]:
        """
        ä»Facebookå¸–å­æŠ“è¯„è®ºï¼ˆæ ¸å¿ƒåŠŸèƒ½2ï¼‰

        Args:
            post_url: å¸–å­URL
            max_comments: æœ€å¤šæŠ“å–è¯„è®ºæ•°

        Returns:
            è¯„è®ºåˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« {username, text, profile_url}
        """
        self._start_browser()

        logger.info(f"ğŸ“– Scraping comments from post...")
        logger.info(f"   URL: {post_url}")

        try:
            # è®¿é—®å¸–å­ï¼ˆä½¿ç”¨domcontentloadedé¿å…crashï¼‰
            self.page.goto(post_url, wait_until='domcontentloaded', timeout=60000)
            time.sleep(random.uniform(3, 5))

            # æ£€æŸ¥ç™»å½•
            if "login" in self.page.url:
                logger.error("âŒ Not logged in")
                return []

            # ç‚¹å‡»"View more comments"æŒ‰é’®
            logger.info("   ğŸ”½ Expanding comments...")
            for _ in range(3):
                try:
                    more_selectors = [
                        'div[role="button"]:has-text("View more comments")',
                        'div[role="button"]:has-text("æŸ¥çœ‹æ›´å¤šè¯„è®º")',
                        'span:has-text("View more comments")',
                    ]

                    for selector in more_selectors:
                        try:
                            more_btn = self.page.query_selector(selector)
                            if more_btn:
                                more_btn.click()
                                time.sleep(random.uniform(1, 2))
                                break
                        except:
                            continue
                except:
                    break

            # æ»šåŠ¨åŠ è½½è¯„è®º
            logger.info("   ğŸ“œ Scrolling to load comments...")
            for _ in range(5):
                self.page.evaluate("window.scrollBy(0, 500)")
                time.sleep(random.uniform(0.5, 1))

            # æŠ“å–è¯„è®º
            comments = []
            seen = set()

            # Facebookè¯„è®ºé€šå¸¸åœ¨ div[role="article"] ä¸­
            comment_elements = self.page.query_selector_all('div[role="article"]')

            logger.info(f"   Found {len(comment_elements)} potential comments")

            for elem in comment_elements[:max_comments * 2]:  # å¤šæŠ“ä¸€äº›ï¼Œå»é‡åå†é™åˆ¶
                try:
                    # æå–ç”¨æˆ·åå’Œé“¾æ¥
                    author_link = elem.query_selector('a[role="link"]')
                    if not author_link:
                        continue

                    username = author_link.inner_text().strip()
                    profile_url = author_link.get_attribute('href') or ''

                    # è¡¥å…¨profile URL
                    if profile_url and profile_url.startswith('/'):
                        profile_url = f'https://www.facebook.com{profile_url}'

                    # æå–è¯„è®ºæ–‡æœ¬
                    text_elem = elem.query_selector('div[dir="auto"]')
                    if not text_elem:
                        continue

                    text = text_elem.inner_text().strip()

                    # å»é‡å¹¶è¿‡æ»¤
                    if username and text and len(text) > 10 and username not in seen:
                        comments.append({
                            'username': username,
                            'text': text,
                            'profile_url': profile_url,
                            'platform': 'facebook',
                            'post_url': post_url
                        })
                        seen.add(username)

                        if len(comments) >= max_comments:
                            break

                except Exception as e:
                    logger.debug(f"Error extracting comment: {e}")
                    continue

            logger.info(f"âœ… Extracted {len(comments)} comments")
            return comments

        except Exception as e:
            logger.error(f"âŒ Error: {e}")
            import traceback
            traceback.print_exc()
            return []

    def search_users(self, group_ids: List[str], limit: int = 100) -> List[Dict]:
        """
        å®Œæ•´æµç¨‹ï¼šè®¿é—®ç¾¤ç»„ â†’ æ‰¾å¸–å­ â†’ æŠ“ç”¨æˆ·
        ï¼ˆé¿å¼€æœç´¢é¡µé¢ï¼Œæ”¹ç”¨ç¾¤ç»„ï¼‰

        Args:
            group_ids: Facebookç¾¤ç»„IDåˆ—è¡¨ï¼ˆä¾‹å¦‚ ["jobsearch", "careeradvice"]ï¼‰
            limit: ç›®æ ‡ç”¨æˆ·æ•°é‡

        Returns:
            ç”¨æˆ·åˆ—è¡¨ï¼ˆä»è¯„è®ºä¸­æå–ï¼‰
        """
        logger.info(f"\nğŸ” Starting Facebook user search...")
        logger.info(f"   Groups: {', '.join(group_ids)}")
        logger.info(f"   Target: {limit} users")

        all_users = []
        seen_usernames = set()

        # Step 1: ä»ç¾¤ç»„è·å–å¸–å­
        posts = self.search_posts_from_groups(group_ids, max_posts_per_group=5)

        if not posts:
            logger.warning("   âš ï¸  No posts found in groups")
            return []

        logger.info(f"\nâœ… Found {len(posts)} posts, now extracting users...")

        # Step 2: ä»æ¯ä¸ªå¸–å­æŠ“å–ç”¨æˆ·
        for i, post in enumerate(posts, 1):
            if len(all_users) >= limit:
                break

            logger.info(f"\n   [{i}/{len(posts)}] Post: {post['url'][:60]}...")

            try:
                # æŠ“å–è¯„è®ºï¼ˆå¾—åˆ°ç”¨æˆ·ï¼‰
                comments = self.get_post_comments(post['url'], max_comments=30)

                # è½¬æ¢è¯„è®ºä¸ºç”¨æˆ·æ ¼å¼
                for comment in comments:
                    username = comment['username']

                    if username in seen_usernames:
                        continue

                    seen_usernames.add(username)

                    # æ„é€ ç”¨æˆ·æ•°æ®
                    user = {
                        'username': username,
                        'profile_url': comment.get('profile_url', ''),
                        'bio': comment.get('text', '')[:200],  # ç”¨è¯„è®ºä½œä¸ºbio
                        'platform': 'facebook',
                        'found_via': f"Comment in group {post.get('group_id', 'unknown')}",
                        'comment_text': comment.get('text', ''),
                        'post_url': post['url']
                    }

                    all_users.append(user)

                    if len(all_users) >= limit:
                        break

            except Exception as e:
                logger.error(f"   âŒ Error: {e}")
                continue

            # å¸–å­é—´å»¶è¿Ÿ
            if i < len(posts):
                time.sleep(random.uniform(2, 4))

        logger.info(f"\nâœ… Total users found: {len(all_users)}")
        return all_users[:limit]

    def get_user_profile(self, user_id: str) -> Dict:
        """
        è·å–ç”¨æˆ·è¯¦ç»†èµ„æ–™ï¼ˆæŠ½è±¡æ–¹æ³•å®ç°ï¼‰
        æ³¨ï¼šFacebook DMç³»ç»Ÿä¸éœ€è¦æ­¤åŠŸèƒ½ï¼Œä»…ä¸ºæ»¡è¶³åŸºç±»è¦æ±‚
        """
        logger.warning("âš ï¸  get_user_profile not implemented for Facebook DM system")
        return {}

    def extract_email(self, user_profile: Dict) -> Optional[str]:
        """
        ä»ç”¨æˆ·èµ„æ–™ä¸­æå–é‚®ç®±ï¼ˆæŠ½è±¡æ–¹æ³•å®ç°ï¼‰
        æ³¨ï¼šFacebook DMç³»ç»Ÿä¸éœ€è¦æ­¤åŠŸèƒ½ï¼Œä»…ä¸ºæ»¡è¶³åŸºç±»è¦æ±‚
        """
        logger.warning("âš ï¸  extract_email not implemented for Facebook DM system")
        return None

    def __del__(self):
        """æ¸…ç†"""
        self._close_browser()


if __name__ == "__main__":
    scraper = FacebookScraper()

    # æµ‹è¯•1: æœç´¢å¸–å­
    print("\n" + "="*70)
    print("Test 1: Search Posts")
    print("="*70)

    posts = scraper.search_posts("job interview tips", max_posts=3)
    for i, post in enumerate(posts, 1):
        print(f"\n[{i}] Author: {post['author']}")
        print(f"    URL: {post['url']}")

    # æµ‹è¯•2: å®Œæ•´æµç¨‹ - æœç´¢ç”¨æˆ·
    print("\n" + "="*70)
    print("Test 2: Search Users (Full Pipeline)")
    print("="*70)

    users = scraper.search_users(
        keywords=["interview preparation", "career advice"],
        limit=20
    )

    for i, user in enumerate(users, 1):
        print(f"\n[{i}] {user['username']}")
        print(f"    Profile: {user['profile_url']}")
        print(f"    Comment: {user['comment_text'][:100]}...")

    scraper._close_browser()
